{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd17ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import pprint\n",
    "loader = PyPDFLoader('attention.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b37b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a01dd754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "('[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, '\n",
      " 'Holger Schwenk,\\n'\n",
      " 'and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder '\n",
      " 'for statistical\\n'\n",
      " 'machine translation. CoRR, abs/1406.1078, 2014.\\n'\n",
      " '[6] Francois Chollet. Xception: Deep learning with depthwise separable '\n",
      " 'convolutions. arXiv\\n'\n",
      " 'preprint arXiv:1610.02357, 2016.\\n'\n",
      " '[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. '\n",
      " 'Empirical evaluation\\n'\n",
      " 'of gated recurrent neural networks on sequence modeling. CoRR, '\n",
      " 'abs/1412.3555, 2014.\\n'\n",
      " '[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. '\n",
      " 'Recurrent neural\\n'\n",
      " 'network grammars. In Proc. of NAACL, 2016.\\n'\n",
      " '[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. '\n",
      " 'Dauphin. Convolu-\\n'\n",
      " 'tional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, '\n",
      " '2017.\\n'\n",
      " '[10] Alex Graves. Generating sequences with recurrent neural networks. arXiv '\n",
      " 'preprint\\n'\n",
      " 'arXiv:1308.0850, 2013.\\n'\n",
      " '[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual '\n",
      " 'learning for im-\\n'\n",
      " 'age recognition. In Proceedings of the IEEE Conference on Computer Vision '\n",
      " 'and Pattern\\n'\n",
      " 'Recognition, pages 770–778, 2016.\\n'\n",
      " '[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. '\n",
      " 'Gradient flow in\\n'\n",
      " 'recurrent nets: the difficulty of learning long-term dependencies, 2001.\\n'\n",
      " '[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural '\n",
      " 'computation,\\n'\n",
      " '9(8):1735–1780, 1997.\\n'\n",
      " '[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with '\n",
      " 'latent annotations\\n'\n",
      " 'across languages. In Proceedings of the 2009 Conference on Empirical Methods '\n",
      " 'in Natural\\n'\n",
      " 'Language Processing, pages 832–841. ACL, August 2009.\\n'\n",
      " '[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and '\n",
      " 'Yonghui Wu. Exploring\\n'\n",
      " 'the limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n'\n",
      " '[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In '\n",
      " 'Advances in Neural\\n'\n",
      " 'Information Processing Systems, (NIPS), 2016.\\n'\n",
      " '[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In '\n",
      " 'International Conference\\n'\n",
      " 'on Learning Representations (ICLR), 2016.\\n'\n",
      " '[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, '\n",
      " 'Alex Graves, and Ko-\\n'\n",
      " 'ray Kavukcuoglu. Neural machine translation in linear time.arXiv preprint '\n",
      " 'arXiv:1610.10099v2,\\n'\n",
      " '2017.\\n'\n",
      " '[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured '\n",
      " 'attention networks.\\n'\n",
      " 'In International Conference on Learning Representations, 2017.\\n'\n",
      " '[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic '\n",
      " 'optimization. In ICLR, 2015.\\n'\n",
      " '[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM '\n",
      " 'networks. arXiv preprint\\n'\n",
      " 'arXiv:1703.10722, 2017.\\n'\n",
      " '[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing '\n",
      " 'Xiang, Bowen\\n'\n",
      " 'Zhou, and Yoshua Bengio. A structured self-attentive sentence embedding. '\n",
      " 'arXiv preprint\\n'\n",
      " 'arXiv:1703.03130, 2017.\\n'\n",
      " '[23] Minh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol Vinyals, and '\n",
      " 'Lukasz Kaiser. Multi-task\\n'\n",
      " 'sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n'\n",
      " '[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective '\n",
      " 'approaches to attention-\\n'\n",
      " 'based neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n'\n",
      " '11')\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))  # Print the number of documents loaded\n",
    "pprint.pp(docs[10].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35ba92ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'author': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'attention.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗ †\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗ ‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023')\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc012a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 1.2.0\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://docs.langchain.com/\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.10/site-packages\n",
      "Requires: langchain-core, langgraph, pydantic\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dd42678",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "818773cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60f4e40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-community\n",
      "Version: 0.4.1\n",
      "Summary: Community contributed LangChain integrations.\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.10/site-packages\n",
      "Requires: aiohttp, dataclasses-json, httpx-sse, langchain-classic, langchain-core, langsmith, numpy, pydantic-settings, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11315316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18c9708e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       ID              SIZE      MODIFIED     \n",
      "nomic-embed-text:latest    0a109f422b47    274 MB    23 hours ago    \n",
      "llama3.2:latest            a80c4f17acd5    2.0 GB    5 weeks ago     \n",
      "llama3.2:1b                baf6a787fdff    1.3 GB    5 weeks ago     \n"
     ]
    }
   ],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8725685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs[:20], embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f47c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"An attention function can be described as a mapping from a query\"\n",
    "results = db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98453514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('mechanism. We propose a new simple network architecture, the Transformer,\\n'\n",
      " 'based solely on attention mechanisms, dispensing with recurrence and '\n",
      " 'convolutions\\n'\n",
      " 'entirely. Experiments on two machine translation tasks show these models to\\n'\n",
      " 'be superior in quality while being more parallelizable and requiring '\n",
      " 'significantly\\n'\n",
      " 'less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\n'\n",
      " 'to-German translation task, improving over the existing best results, '\n",
      " 'including\\n'\n",
      " 'ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation '\n",
      " 'task,\\n'\n",
      " 'our model establishes a new single-model state-of-the-art BLEU score of 41.8 '\n",
      " 'after\\n'\n",
      " 'training for 3.5 days on eight GPUs, a small fraction of the training costs '\n",
      " 'of the\\n'\n",
      " 'best models from the literature. We show that the Transformer generalizes '\n",
      " 'well to\\n'\n",
      " 'other tasks by applying it successfully to English constituency parsing both '\n",
      " 'with\\n'\n",
      " 'large and limited training data.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "caaf4262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-ollama\n",
      "Version: 1.0.1\n",
      "Summary: An integration package connecting Ollama and LangChain\n",
      "Home-page: https://docs.langchain.com/oss/python/integrations/providers/ollama\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.10/site-packages\n",
      "Requires: langchain-core, ollama\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2098f223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama Models: ['nomic-embed-text:latest', 'llama3.2:latest', 'llama3.2:1b']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434\" # Default Ollama URL\n",
    "\n",
    "def get_ollama_models():\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_URL}/api/tags\")\n",
    "        response.raise_for_status() # Raise an exception for bad status codes\n",
    "        data = response.json()\n",
    "        models = [model['name'] for model in data.get('models', [])]\n",
    "        return models\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error connecting to Ollama: {e}\")\n",
    "        return []\n",
    "\n",
    "# Get and print the list\n",
    "available_models = get_ollama_models()\n",
    "print(\"Ollama Models:\", available_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bbd5e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama3.2:1b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0db1d2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-core\n",
      "Version: 1.2.4\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://docs.langchain.com/\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.10/site-packages\n",
      "Requires: jsonpatch, langsmith, packaging, pydantic, pyyaml, tenacity, typing-extensions, uuid-utils\n",
      "Required-by: langchain, langchain-classic, langchain-community, langchain-ollama, langchain-text-splitters, langgraph, langgraph-checkpoint, langgraph-prebuilt, langserve\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4a44af24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ChatMessagePromptTemplate\nrole\n  Field required [type=missing, input_value={'prompt': PromptTemplate...              Answer:')}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatMessagePromptTemplate\n\u001b[0;32m----> 2\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[43mChatMessagePromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m                                                 you are a helpful assistant that will help answer user questions\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m                                                 based on the context provided.\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m                                                 answer the questions correctly and i will give rs 200\u001b[39;49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;43m                                                 \u001b[39;49m\u001b[38;5;132;43;01m{context}\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m                                                 Question: \u001b[39;49m\u001b[38;5;132;43;01m{question}\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m                                                 Answer:\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.10/site-packages/langchain_core/prompts/chat.py:254\u001b[0m, in \u001b[0;36mBaseStringMessagePromptTemplate.from_template\u001b[0;34m(cls, template, template_format, partial_variables, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a class from a string template.\u001b[39;00m\n\u001b[1;32m    234\u001b[0m \n\u001b[1;32m    235\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    A new instance of this class.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    249\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[1;32m    250\u001b[0m     template,\n\u001b[1;32m    251\u001b[0m     template_format\u001b[38;5;241m=\u001b[39mtemplate_format,\n\u001b[1;32m    252\u001b[0m     partial_variables\u001b[38;5;241m=\u001b[39mpartial_variables,\n\u001b[1;32m    253\u001b[0m )\n\u001b[0;32m--> 254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.10/site-packages/langchain_core/load/serializable.py:116\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/rag/lib/python3.10/site-packages/pydantic/main.py:250\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    249\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    252\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    253\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    255\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    256\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    257\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatMessagePromptTemplate\nrole\n  Field required [type=missing, input_value={'prompt': PromptTemplate...              Answer:')}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.12/v/missing"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatMessagePromptTemplate\n",
    "prompt = ChatMessagePromptTemplate.from_template(\"\"\"\n",
    "                                                 you are a helpful assistant that will help answer user questions\n",
    "                                                 based on the context provided.\n",
    "                                                 answer the questions correctly and i will give rs 200\n",
    "                                                 {context}\n",
    "                                                 Question: {question}\n",
    "                                                 Answer:\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "09d4b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatMessagePromptTemplate,ChatPromptTemplate\n",
    "\n",
    "\n",
    "# prompt = ChatMessagePromptTemplate.from_template(template=\"\"\"\n",
    "# You are a helpful assistant that helps people find information.\n",
    "# Answer the following question based on the context provided.\n",
    "\n",
    "# <context>\n",
    "# {context}\n",
    "# </context>\n",
    "\n",
    "# Question: {question}\n",
    "# \"\"\",\n",
    "#     role=\"system\")\n",
    "\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Answer the user's question based on the provided context: {context}\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "                                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "621af1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm = llm, prompt = prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a308053e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x10cf1f4c0>, search_kwargs={})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=db.as_retriever(document_chain=document_chain)\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9751ee16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever,document_chain)\n",
    "\n",
    "query = \"Explain attention mechanism in neural networks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6fb4679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = retrieval_chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f7d91cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The Transformer model proposed by Ashish Vaswani, Noam Shazeer, Niki Parmar, '\n",
      " 'Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia '\n",
      " 'Polosukhin is a new simple network architecture based solely on attention '\n",
      " 'mechanisms, without the need for recurrence or convolutional neural '\n",
      " 'networks.\\n'\n",
      " '\\n'\n",
      " 'In essence, this means that instead of processing sequential data in rows '\n",
      " '(recurrence) or columns (convolutions), the Transformer model processes the '\n",
      " 'input sequence as a whole, using self-attention mechanisms to weigh the '\n",
      " 'importance of different parts of the input. This allows for parallelization '\n",
      " 'and significant reduction in training time compared to traditional models.\\n'\n",
      " '\\n'\n",
      " 'The experiments conducted on two machine translation tasks showed that this '\n",
      " 'approach can achieve superior quality while being more parallelizable and '\n",
      " 'requiring less time to train than existing state-of-the-art models from the '\n",
      " 'literature.')\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(answer['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d325d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
